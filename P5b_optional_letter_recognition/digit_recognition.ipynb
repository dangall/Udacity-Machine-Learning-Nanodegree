{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Engineer Nanodegree\n",
    "## Deep Learning\n",
    "## Project: Build a Digit Recognition Program\n",
    "\n",
    "In this notebook, a template is provided for you to implement your functionality in stages which is required to successfully complete this project. If additional code is required that cannot be included in the notebook, be sure that the Python code is successfully imported and included in your submission, if necessary. Sections that begin with **'Implementation'** in the header indicate where you should begin your implementation for your project. Note that some sections of implementation are optional, and will be marked with **'Optional'** in the header.\n",
    "\n",
    "In addition to implementing code, there will be questions that you must answer which relate to the project and your implementation. Each section where you will answer a question is preceded by a **'Question'** header. Carefully read each question and provide thorough answers in the following text boxes that begin with **'Answer:'**. Your project submission will be evaluated based on your answers to each of the questions and the implementation you provide.\n",
    "\n",
    ">**Note:** Code and Markdown cells can be executed using the **Shift + Enter** keyboard shortcut. In addition, Markdown cells can be edited by typically double-clicking the cell to enter edit mode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Step 1: Design and Test a Model Architecture\n",
    "Design and implement a deep learning model that learns to recognize sequences of digits. Train the model using synthetic data generated by concatenating character images from [notMNIST](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html) or [MNIST](http://yann.lecun.com/exdb/mnist/). To produce a synthetic sequence of digits for testing, you can for example limit yourself to sequences up to five digits, and use five classifiers on top of your deep network. You would have to incorporate an additional ‘blank’ character to account for shorter number sequences.\n",
    "\n",
    "There are various aspects to consider when thinking about this problem:\n",
    "- Your model can be derived from a deep neural net or a convolutional network.\n",
    "- You could experiment sharing or not the weights between the softmax classifiers.\n",
    "- You can also use a recurrent network in your deep neural net to replace the classification layers and directly emit the sequence of digits one-at-a-time.\n",
    "\n",
    "You can use ** Keras ** to implement your model. Read more at [keras.io](https://keras.io/).\n",
    "\n",
    "Here is an example of a [published baseline model on this problem](http://static.googleusercontent.com/media/research.google.com/en//pubs/archive/42241.pdf). ([video](https://www.youtube.com/watch?v=vGPI_JvLoN0)). You are not expected to model your architecture precisely using this model nor get the same performance levels, but this is more to show an exampe of an approach used to solve this particular problem. We encourage you to try out different architectures for yourself and see what works best for you. Here is a useful [forum post](https://discussions.udacity.com/t/goodfellow-et-al-2013-architecture/202363) discussing the architecture as described in the paper and here is [another one](https://discussions.udacity.com/t/what-loss-function-to-use-for-multi-digit-svhn-training/176897) discussing the loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "Use the code cell (or multiple code cells, if necessary) to implement the first step of your project. Once you have completed your implementation and are satisfied with the results, be sure to thoroughly answer the questions that follow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the data for notMNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from urllib.request import urlretrieve\n",
    "import tarfile\n",
    "from os.path import isdir, isfile\n",
    "from os import remove\n",
    "\n",
    "def folder_file_name(urlpath):\n",
    "    \"\"\"\n",
    "    Takes a URL and returns the characters after the final '/' as\n",
    "    the filename. In the filename, everything up until the first \n",
    "    period is declared to be the foldername, i.e. the folder-name\n",
    "    to which a tar file would be unpackaged.\n",
    "    \"\"\"\n",
    "    # We first find the index where the name of the file begins\n",
    "    indexname = urlpath.rfind(\"/\")\n",
    "    # Now we get the file name\n",
    "    filename = urlpath[indexname+1:]\n",
    "    # We will do the same for the folder name\n",
    "    folderindex = filename.find(\".\")\n",
    "    foldername = filename[:folderindex]\n",
    "    return filename, foldername\n",
    "\n",
    "def download_and_unpackage(urlpath):\n",
    "    \"\"\"\n",
    "    Downloads a file given by the url address and unpackages it\n",
    "    into the same directory. It then removes the compressed file.\n",
    "    \"\"\"\n",
    "    filename, foldername = folder_file_name(urlpath)\n",
    "    # We only want to download and unpackage if we haven't already done it\n",
    "    if isdir(foldername) != True:\n",
    "        urlretrieve(urlpath, filename)\n",
    "        with tarfile.open(filename, mode='r:gz') as compressed_file:\n",
    "            compressed_file.extractall()\n",
    "            compressed_file.close()\n",
    "        remove(filename)\n",
    "\n",
    "# Now the data for notMNIST\n",
    "download_and_unpackage(\"http://commondatastorage.googleapis.com/books1000/notMNIST_small.tar.gz\")\n",
    "download_and_unpackage(\"http://commondatastorage.googleapis.com/books1000/notMNIST_large.tar.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy.ndimage\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find path names of files and prepare one-hot encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We first get all path-names for the training and testing images\n",
    "training_pathnames = [[folderandfiles[0]+\"/\"+imname for imname in folderandfiles[2]] \n",
    "                          for folderandfiles in os.walk(\"./notMNIST_large\") if folderandfiles[2]!=[]]\n",
    "testing_pathnames = [[folderandfiles[0]+\"/\"+imname for imname in folderandfiles[2]] \n",
    "                          for folderandfiles in os.walk(\"./notMNIST_small\") if folderandfiles[2]!=[]]\n",
    "# training_pathnames has the structure:\n",
    "#[[list of path-names in first folder], [list of paths in second folder], ...]\n",
    "\n",
    "def get_letter(filepath):\n",
    "    \"\"\"\n",
    "    Returns the letter corresponding to an image found in filepath\n",
    "    \"\"\"\n",
    "    # The letter is given by the name of the folder in which we find the image\n",
    "    indexname = filepath.rfind(\"/\")\n",
    "    letter = filepath[indexname-1:indexname]\n",
    "    return letter\n",
    "\n",
    "# In each folder all images depict the same letter\n",
    "all_letters = np.sort([get_letter(pathname[0]) for pathname in training_pathnames])\n",
    "\n",
    "# We may now make the function that one-hot-encodes letters into arrays\n",
    "enc = LabelBinarizer()\n",
    "enc.fit(all_letters)\n",
    "\n",
    "def one_hot_encode(list_of_letters):\n",
    "    \"\"\"\n",
    "    One hot encode a list of letters. Returns a one-hot encoded vector for each letter.\n",
    "    \"\"\"\n",
    "    return enc.transform(list_of_letters)\n",
    "\n",
    "# We now flatten the lists of path names\n",
    "training_pathnames = np.array(sum(training_pathnames, []))\n",
    "testing_pathnames = np.array(sum(testing_pathnames, []))\n",
    "\n",
    "# When trainig, we don't want theimages to be ordered. Therefore, we take a \n",
    "# random permutation of their order.\n",
    "np.random.seed(42)\n",
    "training_pathnames = np.random.permutation(training_pathnames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load images and labels into arrays, save to disk\n",
    "\n",
    "We first normalize the pixel-values to lie between 0 and 1. We also reshape each image to be a 3-dimensional array: (x_length, y_length, color_channels). \n",
    "\n",
    "Finally, we save the arrays to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_normalize_image(path):\n",
    "    \"\"\"\n",
    "    Takes the directory path of an image and returns a normalized\n",
    "    3-dimensional array representing that image.\n",
    "    \"\"\"\n",
    "    # First we load the image\n",
    "    try:\n",
    "        imagearray = scipy.ndimage.imread(path)\n",
    "        # Now we normalize it \n",
    "        imagearray = imagearray / 255\n",
    "        # We reshape it to be 3-dimensional: x-dim, y-dim, num_colors\n",
    "        imagearray = imagearray.reshape(imagearray.shape + (1,))\n",
    "        return imagearray\n",
    "    except:\n",
    "        # Some images are broken in the database; these will raise errors.\n",
    "        pass\n",
    "    \n",
    "def array_all_images(list_of_path_names):\n",
    "    \"\"\"\n",
    "    Takes a list of directory paths to images and returns a 4-dimensional array\n",
    "    containing the pixel-data of those images. The shape is:\n",
    "    (num_images, x_dim, y_dim, num_colors)\n",
    "    \"\"\"\n",
    "    all_images = [load_normalize_image(path) for path in list_of_path_names]\n",
    "    # Some of these might be None since the function load_normalize_image\n",
    "    # does not load broken images. We now remove these Nones.\n",
    "    all_images = np.array(list(filter(None.__ne__, all_images)))\n",
    "    return all_images\n",
    "\n",
    "def load_letter(path):\n",
    "    \"\"\"\n",
    "    Takes the directory path of an image and returns a the letter-label of the image.\n",
    "    \"\"\"\n",
    "    # First we see if it is possible to load the image\n",
    "    try:\n",
    "        imagearray = scipy.ndimage.imread(path)\n",
    "        # If this didn't give an error, we may get the letter\n",
    "        return get_letter(path)\n",
    "    except:\n",
    "        # Some images are broken in the database; these will raise errors.\n",
    "        pass\n",
    "\n",
    "def array_all_labels(list_of_path_names):\n",
    "    \"\"\"\n",
    "    Takes a list of directory paths to images and returns a 2-dimensional array\n",
    "    containing the one-hot-encoded labels of those images\n",
    "    \"\"\"\n",
    "    the_letters = [load_letter(path) for path in list_of_path_names]\n",
    "    the_letters = list(filter(None.__ne__, the_letters))\n",
    "    all_labels = one_hot_encode(the_letters)\n",
    "    return all_labels\n",
    "\n",
    "def batch_list(inputlist, batch_size):\n",
    "    \"\"\"\n",
    "    Returns the inputlist split into batches of maximal length batch_size.\n",
    "    Each element in the returned list (i.e. each batch) is itself a list.\n",
    "    \"\"\"\n",
    "    list_of_batches = [inputlist[ii: ii+batch_size] for ii in range(0, len(inputlist), batch_size)]\n",
    "    return list_of_batches\n",
    "\n",
    "# We store all the data in a training and testing folder\n",
    "if not os.path.exists(\"training_data\"):\n",
    "    os.makedirs(\"training_data\")\n",
    "if not os.path.exists(\"testing_data\"):\n",
    "    os.makedirs(\"testing_data\")\n",
    "    \n",
    "# Make the input data and labels for the testing set\n",
    "if isfile(\"./testing_data/testing_images.npy\") == False:\n",
    "    testing_images = array_all_images(testing_pathnames)\n",
    "    np.save(\"./testing_data/testing_images.npy\", testing_images)\n",
    "if isfile(\"./testing_data/testing_labels.npy\") == False:\n",
    "    testing_labels = array_all_labels(testing_pathnames)\n",
    "    np.save(\"./testing_data/testing_labels.npy\", testing_labels)\n",
    "\n",
    "# The trainining examples need to be turned into batches\n",
    "def batch_list(inputlist, batch_size):\n",
    "    \"\"\"\n",
    "    Returns the inputlist split into batches of maxmial length batch_size.\n",
    "    Each element in the returned list (i.e. each batch) is itself a list.\n",
    "    \"\"\"\n",
    "    list_of_batches = [inputlist[ii: ii+batch_size] for ii in range(0, len(inputlist), batch_size)]\n",
    "    return list_of_batches\n",
    "\n",
    "# Here we specify the size of each batch\n",
    "batch_size = 2**12\n",
    "\n",
    "# Now we save the batch-data, unless it already exists\n",
    "training_pathnames_batches = batch_list(training_pathnames, batch_size)\n",
    "num_saved_batches = sum([\"training_images_batch\" in filename \n",
    "                         for filename in list(os.walk(\"./training_data\"))[0][2]])\n",
    "\n",
    "# If we have a different number of batches saved comapred to what we want,\n",
    "# the batches are wrong and need recomputing.\n",
    "if num_saved_batches != len(training_pathnames_batches):\n",
    "    # We could delete the old files, but this is dangerous, since a \n",
    "    # typo could remove all files on the computer. We simply overwrite the files we have\n",
    "    for ii, batch in enumerate(tqdm(training_pathnames_batches)):\n",
    "        training_images_batch = array_all_images(batch)\n",
    "        np.save(\"./training_data/training_images_batch\" + str(ii) + \".npy\", training_images_batch)\n",
    "\n",
    "        training_labels_batch = array_all_labels(batch)\n",
    "        np.save(\"./training_data/training_labels_batch\" + str(ii) + \".npy\", training_labels_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and display an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape: (28, 28, 1)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCRER8zQTAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6258172eb8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def load_training_data(batch_number, image_numbers=[]):\n",
    "    \"\"\"\n",
    "    Loads the training data from files. It is possible to specify an interval \n",
    "    of images to load, or by defauly load th entire batch.\n",
    "    \"\"\"\n",
    "    if image_numbers == []:\n",
    "        return np.load(\"./training_data/training_images_batch\" + str(batch_number) + \".npy\")\n",
    "    else:\n",
    "        return np.load(\"./training_data/training_images_batch\" + str(batch_number) + \".npy\")[image_numbers]\n",
    "    \n",
    "def load_training_labels(batch_number, image_numbers=[]):\n",
    "    \"\"\"\n",
    "    Loads the training data from files. It is possible to specify an interval \n",
    "    of images to load, or by defauly load th entire batch.\n",
    "    \"\"\"\n",
    "    if image_numbers == []:\n",
    "        return np.load(\"./training_data/training_labels_batch\" + str(batch_number) + \".npy\")\n",
    "    else:\n",
    "        return np.load(\"./training_data/training_labels_batch\" + str(batch_number) + \".npy\")[image_numbers]\n",
    "    \n",
    "def display_image(imagearray):\n",
    "    array_to_plot = imagearray.reshape((imagearray.shape[0], imagearray.shape[1]))\n",
    "    print(\"Image shape: {}\".format(imagearray.shape))\n",
    "    plt.imshow(array_to_plot, cmap=\"gray\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    \n",
    "display_image(load_training_data(0)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow Neural Network\n",
    "\n",
    "This code significantly overlaps with the code from Project 5: Image Classification, in my [GitHub folder](https://github.com/dangall/Udacity-Machine-Learning-Nanodegree/blob/master/P5_image_classification/image_classification.ipynb).\n",
    "\n",
    "### Set up functions necessary to build the neural network\n",
    "\n",
    "#### Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def neural_net_image_input(image_shape):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of image input\n",
    "    : image_shape: Shape of the images\n",
    "    : return: Tensor for image input.\n",
    "    \"\"\"\n",
    "    return tf.placeholder(tf.float32, [None, image_shape[0], image_shape[1], image_shape[2]], name=\"x\")\n",
    "\n",
    "\n",
    "def neural_net_label_input(n_classes):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of label input\n",
    "    : n_classes: Number of classes\n",
    "    : return: Tensor for label input.\n",
    "    \"\"\"\n",
    "    return tf.placeholder(tf.float32, [None, n_classes], name=\"y\")\n",
    "\n",
    "\n",
    "def neural_net_keep_prob_input():\n",
    "    \"\"\"\n",
    "    Return a Tensor for keep probability\n",
    "    \"\"\"\n",
    "    return tf.placeholder(tf.float32, name=\"keep_prob\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convolution and Max Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides):\n",
    "    \"\"\"\n",
    "    Apply convolution then max pooling to x_tensor\n",
    "    :param x_tensor: TensorFlow Tensor\n",
    "    :param conv_num_outputs: Number of outputs for the convolutional layer\n",
    "    :param conv_ksize: kernal size 2-D Tuple for the convolutional layer\n",
    "    :param conv_strides: Stride 2-D Tuple for convolution\n",
    "    :param pool_ksize: kernal size 2-D Tuple for pool\n",
    "    :param pool_strides: Stride 2-D Tuple for pool\n",
    "    : return: A tensor that represents convolution and max pooling of x_tensor\n",
    "    \"\"\"    \n",
    "    # Number of input colors\n",
    "    num_inputcolors = x_tensor.shape.as_list()[3]\n",
    "    \n",
    "    # Convolutional filter\n",
    "    W_conv= tf.Variable(tf.truncated_normal([conv_ksize[0], conv_ksize[1], num_inputcolors, conv_num_outputs], stddev=0.1))\n",
    "    b_conv = tf.Variable(tf.constant(0.1, shape=[conv_num_outputs]))\n",
    "    \n",
    "    convolution = tf.nn.conv2d(x_tensor, W_conv, strides=[1, conv_strides[0], conv_strides[1], 1], padding='SAME')\n",
    "    h_conv = tf.nn.relu(convolution + b_conv)\n",
    "    \n",
    "    h_pool = tf.nn.max_pool(h_conv, ksize=[1, pool_ksize[0], pool_ksize[1], 1], \n",
    "                            strides=[1, pool_strides[0], pool_strides[1], 1], padding='SAME')\n",
    "    \n",
    "    return h_pool "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Flatten Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def flatten(x_tensor):\n",
    "    \"\"\"\n",
    "    Flatten x_tensor to (Batch Size, Flattened Image Size)\n",
    "    : x_tensor: A tensor of size (Batch Size, ...), where ... are the image dimensions.\n",
    "    : return: A tensor of size (Batch Size, Flattened Image Size).\n",
    "    \"\"\"    \n",
    "    flat_dimension = np.prod(x_tensor.shape.as_list()[1:])\n",
    "    x_flat = tf.reshape(x_tensor, [-1, flat_dimension])\n",
    "\n",
    "    return x_flat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fully-Connected Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fully_conn(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a fully connected layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"    \n",
    "    input_dimensions = x_tensor.shape.as_list()[1]    \n",
    "    W = tf.Variable(tf.truncated_normal([input_dimensions, num_outputs], stddev=0.1))\n",
    "    b = tf.Variable(tf.constant(0.1, shape=[num_outputs]))\n",
    "    \n",
    "    h_connected = tf.nn.relu(tf.matmul(x_tensor, W) + b)\n",
    "    \n",
    "    return h_connected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def output(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a output layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    input_dimensions = x_tensor.shape.as_list()[1]    \n",
    "    W = tf.Variable(tf.truncated_normal([input_dimensions, num_outputs], stddev=0.1))\n",
    "    b = tf.Variable(tf.constant(0.1, shape=[num_outputs]))\n",
    "    \n",
    "    h_output = tf.matmul(x_tensor, W) + b\n",
    "    \n",
    "    return h_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "    # Function Definition from Above:\n",
    "    #    conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    convlayer_1 = tf.nn.dropout(conv2d_maxpool(x, 20, (4, 4), (1, 1), (2, 2), (2, 2)), keep_prob)\n",
    "    #convlayer_1b = tf.nn.dropout(conv2d_maxpool(convlayer_1, 10, (4, 4), (1, 1), (2, 2), (1, 1)), keep_prob)\n",
    "    \n",
    "    convlayer_2 = tf.nn.dropout(conv2d_maxpool(convlayer_1, 30, (4, 4), (1, 1), (2, 2), (2, 2)), keep_prob)\n",
    "    #convlayer_2b = conv2d_maxpool(convlayer_2, 20, (1, 1), (1, 1), (1, 1), (1, 1))\n",
    "    \n",
    "    convlayer_3 = tf.nn.dropout(conv2d_maxpool(convlayer_2, 60, (4, 4), (1, 1), (2, 2), (2, 2)), keep_prob)\n",
    "    #convlayer_3b = conv2d_maxpool(convlayer_3, 50, (1, 1), (1, 1), (1, 1), (1, 1))\n",
    "\n",
    "    # Function Definition from Above:\n",
    "    #   flatten(x_tensor)\n",
    "    flattened_tensor = flatten(convlayer_3)\n",
    "    \n",
    "    # Function Definition from Above:\n",
    "    #   fully_conn(x_tensor, num_outputs)\n",
    "    connlayer_1 = tf.nn.dropout(fully_conn(flattened_tensor, 200), keep_prob)\n",
    "    \n",
    "    connlayer_2 = tf.nn.dropout(fully_conn(connlayer_1, 100), keep_prob)\n",
    "    \n",
    "    connlayer_3 = tf.nn.dropout(fully_conn(connlayer_2, 30), keep_prob)\n",
    "    \n",
    "    # Function Definition from Above:\n",
    "    #   output(x_tensor, num_outputs)\n",
    "    outputlayer = output(connlayer_3, 10)\n",
    "\n",
    "    return outputlayer\n",
    "\n",
    "#=============================\n",
    "#  Build the Neural Network\n",
    "#=============================\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((28, 28, 1))\n",
    "y = neural_net_label_input(10)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "#logits = conv_net(x, keep_prob)\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Train the neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs = 30\n",
    "keep_probability = 0.5\n",
    "learning_rate = 0.0001 # default is 0.001 N.B. it is also possible to make this a placeholder object!\n",
    "size_of_minibatch = 2**7\n",
    "\n",
    "#===============================\n",
    "# Don't need to edit below this\n",
    "#===============================\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print cost and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The final batch will be our validation set\n",
    "validation_inputarray = load_training_data(num_saved_batches - 1)\n",
    "validation_labels = load_training_labels(num_saved_batches - 1)\n",
    "\n",
    "def get_stats(session, feature_batch, label_batch, cost, accuracy, printout=True):\n",
    "    \"\"\"\n",
    "    Obtain information about loss and validation accuracy\n",
    "    : session: Current TensorFlow session\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    : cost: TensorFlow cost function\n",
    "    : accuracy: TensorFlow accuracy function\n",
    "    \"\"\"\n",
    "    cost_value = session.run(cost, feed_dict={x: feature_batch, y: label_batch, keep_prob:1.0})\n",
    "    accuracy_value = session.run(accuracy, feed_dict={x: validation_inputarray, \n",
    "                                                      y: validation_labels, keep_prob:1.0})\n",
    "    if printout:\n",
    "        print(\"\\nLoss: {}\".format(cost_value))\n",
    "        print(\"Accuracy (validation): {}\".format(accuracy_value))\n",
    "    return cost_value, accuracy_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choose which of the training or testing cells below to use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train on a single batch\n",
    "\n",
    "In order to pick the best hyperparameters, we begin by training on a single batch. This will tell us when to stop the learning and will help in choosing a learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_i = 0\n",
    "\n",
    "print('Checking the Training on a Single Batch, i.e. number {}'.format(batch_i))\n",
    "\n",
    "accuracy_list = []\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        for batch_inputarrays, batch_labels in zip(batch_list(load_training_data(batch_i), size_of_minibatch),\n",
    "                                                   batch_list(load_training_labels(batch_i), size_of_minibatch)):\n",
    "            sess.run(optimizer, feed_dict={x: batch_inputarrays, y: batch_labels, keep_prob: keep_probability})\n",
    "        cost_value, accuracy_value = get_stats(sess, batch_inputarrays, batch_labels, cost, accuracy, \n",
    "                                               printout=False)\n",
    "        #print('\\nEpoch {:>2}, Batch {}: {} '.format(epoch + 1, batch_i, accuracy_value), end='')\n",
    "        accuracy_list.append(accuracy_value)\n",
    "    \n",
    "    # Save the model\n",
    "    #saver = tf.train.Saver()\n",
    "    #save_path = saver.save(sess, \"./trained_model\", global_step=epoch)\n",
    "        \n",
    "plt.plot(accuracy_list)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fully train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Training...')\n",
    "\n",
    "epochs = 30\n",
    "\n",
    "accuracy_list = []\n",
    "with tf.Session() as sess:\n",
    "    # It is very important the saver is defined INSIDE the block \"with tf.Session() as sess\"\n",
    "    # otherwise it will be very difficult to load the graph (unless we name all the variables etc)\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        for batch_i in range(num_saved_batches - 1):\n",
    "            for batch_inputarrays, batch_labels in zip(batch_list(load_training_data(batch_i), \n",
    "                                                                  size_of_minibatch),\n",
    "                                                       batch_list(load_training_labels(batch_i), \n",
    "                                                                  size_of_minibatch)\n",
    "                                                      ):\n",
    "                sess.run(optimizer, feed_dict={x: batch_inputarrays, y: batch_labels, keep_prob: keep_probability})\n",
    "            cost_value, accuracy_value = get_stats(sess, batch_inputarrays, batch_labels, cost, accuracy, \n",
    "                                                   printout=False)\n",
    "            print('Epoch {:>2}, Batch {}: {}'.format(epoch + 1, batch_i, accuracy_value))\n",
    "        \n",
    "        accuracy_list.append(accuracy_value)\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            # Save the intermediate model\n",
    "            save_path = saver.save(sess, \"./trained_model\", global_step=epoch)\n",
    "    \n",
    "    # Save the final model\n",
    "    save_path = saver.save(sess, \"./trained_model\", global_step=epoch)\n",
    "        \n",
    "plt.plot(accuracy_list)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fine-tune a trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Decrease the learning rate for the final part!\n",
    "\n",
    "epochs = 30\n",
    "load_model = \"./trained_model-29\"\n",
    "\n",
    "# read off the epoch from the number in load_model\n",
    "next_epoch = int(load_model[load_model.rfind(\"-\")+1:]) + 1\n",
    "\n",
    "new_accuracy_list = []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    saver.restore(sess, load_model)\n",
    "    print(sess.run(accuracy, feed_dict={x: validation_inputarray,\n",
    "                                        y: validation_labels, keep_prob:1.0}))\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(next_epoch, next_epoch + epochs):\n",
    "        for batch_i in range(num_saved_batches - 1):\n",
    "            for batch_inputarrays, batch_labels in zip(batch_list(load_training_data(batch_i), \n",
    "                                                                  size_of_minibatch),\n",
    "                                                       batch_list(load_training_labels(batch_i), \n",
    "                                                                  size_of_minibatch)\n",
    "                                                      ):\n",
    "                sess.run(optimizer, feed_dict={x: batch_inputarrays, y: batch_labels, \n",
    "                                                  keep_prob: keep_probability})\n",
    "            cost_value, accuracy_value = get_stats(sess, batch_inputarrays, batch_labels, cost, accuracy,\n",
    "                                                   printout=False)\n",
    "            print('Epoch {:>2}, Batch {}: {}'.format(epoch + 1, batch_i, accuracy_value))\n",
    "        \n",
    "        new_accuracy_list.append(accuracy_value)\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            # Save the intermediate model\n",
    "            save_path = saver.save(sess, \"./trained_model\", global_step=epoch)\n",
    "            \n",
    "    # Save the final model\n",
    "    save_path = saver.save(sess, \"./trained_model\", global_step=epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(new_accuracy_list)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model's test-set accuracty is 96.64%\n"
     ]
    }
   ],
   "source": [
    "load_model = \"./trained_model-59\"\n",
    "testing_inputarray = np.load(\"./testing_data/testing_images.npy\")\n",
    "testing_labels = np.load(\"./testing_data/testing_labels.npy\")\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    saver.restore(sess, load_model)\n",
    "    print(\"The model's test-set accuracty is {}%\".format(np.round(sess.run(accuracy, \n",
    "                                                                            feed_dict={x: testing_inputarray,\n",
    "                                        y: testing_labels, keep_prob:1.0})*100, decimals=2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional future work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "_What approach did you take in coming up with a solution to this problem?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "_What does your final architecture look like? (Type of model, layers, sizes, connectivity, etc.)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "_How did you train your model? How did you generate your synthetic dataset?_ Include examples of images from the synthetic data you constructed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Step 2: Train a Model on a Realistic Dataset\n",
    "Once you have settled on a good architecture, you can train your model on real data. In particular, the [Street View House Numbers (SVHN)](http://ufldl.stanford.edu/housenumbers/) dataset is a good large-scale dataset collected from house numbers in Google Street View. Training on this more challenging dataset, where the digits are not neatly lined-up and have various skews, fonts and colors, likely means you have to do some hyperparameter exploration to perform well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "Use the code cell (or multiple code cells, if necessary) to implement the first step of your project. Once you have completed your implementation and are satisfied with the results, be sure to thoroughly answer the questions that follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "_Describe how you set up the training and testing data for your model. How does the model perform on a realistic dataset?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "_What changes did you have to make, if any, to achieve \"good\" results? Were there any options you explored that made the results worse?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "_What were your initial and final results with testing on a realistic dataset? Do you believe your model is doing a good enough job at classifying numbers correctly?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Step 3: Test a Model on Newly-Captured Images\n",
    "\n",
    "Take several pictures of numbers that you find around you (at least five), and run them through your classifier on your computer to produce example results. Alternatively (optionally), you can try using OpenCV / SimpleCV / Pygame to capture live images from a webcam and run those through your classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "Use the code cell (or multiple code cells, if necessary) to implement the first step of your project. Once you have completed your implementation and are satisfied with the results, be sure to thoroughly answer the questions that follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7\n",
    "_Choose five candidate images of numbers you took from around you and provide them in the report. Are there any particular qualities of the image(s) that might make classification difficult?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8\n",
    "_Is your model able to perform equally well on captured pictures or a live camera stream when compared to testing on the realistic dataset?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: Question 9\n",
    "_If necessary, provide documentation for how an interface was built for your model to load and classify newly-acquired images._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** Leave blank if you did not complete this part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### Step 4: Explore an Improvement for a Model\n",
    "\n",
    "There are many things you can do once you have the basic classifier in place. One example would be to also localize where the numbers are on the image. The SVHN dataset provides bounding boxes that you can tune to train a localizer. Train a regression loss to the coordinates of the bounding box, and then test it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "Use the code cell (or multiple code cells, if necessary) to implement the first step of your project. Once you have completed your implementation and are satisfied with the results, be sure to thoroughly answer the questions that follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 10\n",
    "_How well does your model localize numbers on the testing set from the realistic dataset? Do your classification results change at all with localization included?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 11\n",
    "_Test the localization function on the images you captured in **Step 3**. Does the model accurately calculate a bounding box for the numbers in the images you found? If you did not use a graphical interface, you may need to investigate the bounding boxes by hand._ Provide an example of the localization created on a captured image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Optional Step 5: Build an Application or Program for a Model\n",
    "Take your project one step further. If you're interested, look to build an Android application or even a more robust Python program that can interface with input images and display the classified numbers and even the bounding boxes. You can for example try to build an augmented reality app by overlaying your answer on the image like the [Word Lens](https://en.wikipedia.org/wiki/Word_Lens) app does.\n",
    "\n",
    "Loading a TensorFlow model into a camera app on Android is demonstrated in the [TensorFlow Android demo app](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/android), which you can simply modify.\n",
    "\n",
    "If you decide to explore this optional route, be sure to document your interface and implementation, along with significant results you find. You can see the additional rubric items that you could be evaluated on by [following this link](https://review.udacity.com/#!/rubrics/413/view)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional Implementation\n",
    "Use the code cell (or multiple code cells, if necessary) to implement the first step of your project. Once you have completed your implementation and are satisfied with the results, be sure to thoroughly answer the questions that follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Documentation\n",
    "Provide additional documentation sufficient for detailing the implementation of the Android application or Python program for visualizing the classification of numbers in images. It should be clear how the program or application works. Demonstrations should be provided. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Write your documentation here._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note**: Once you have completed all of the code implementations and successfully answered each question above, you may finalize your work by exporting the iPython Notebook as an HTML document. You can do this by using the menu above and navigating to  \n",
    "**File -> Download as -> HTML (.html)**. Include the finished document along with this notebook as your submission."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 TensorFlow",
   "language": "python",
   "name": "py36tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
